{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malikabriq/smartlearn-fyp/blob/main/my_chatbot_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 - Install dependencies\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q streamlit pyngrok crawl4ai langchain transformers markdownify nest_asyncio psutil sentence-transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXCKZzrRQQP",
        "outputId": "2ae747c8-7c24-4398-83f5-1efdf8f5a20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Write Streamlit app\n",
        "%%writefile app_local_agent_fixed.py\n",
        "import streamlit as st\n",
        "import os, json, asyncio, markdownify, nest_asyncio, traceback, torch\n",
        "import zipfile, io\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from langchain.schema import Document\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Make async work inside Streamlit/Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------- Paths -------------------\n",
        "BASE_PATH = \"/content\"\n",
        "DOCS_DIR = os.path.join(BASE_PATH, \"local_docs\")\n",
        "MD_DIR = os.path.join(BASE_PATH, \"scraped_markdown\")\n",
        "DOC_PATH = os.path.join(DOCS_DIR, \"stored_docs.json\")\n",
        "\n",
        "os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "os.makedirs(MD_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------- Helpers -------------------\n",
        "async def crawl_site_async(url: str, max_pages: int):\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        res = await crawler.arun(url=url, max_pages=max_pages, markdown=True)\n",
        "        return res\n",
        "\n",
        "def save_as_json(docs: List[Document], append=True):\n",
        "    all_docs = []\n",
        "    if append and os.path.exists(DOC_PATH):\n",
        "        try:\n",
        "            with open(DOC_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "                all_docs = json.load(f)\n",
        "        except Exception:\n",
        "            all_docs = []\n",
        "\n",
        "    for d in docs:\n",
        "        all_docs.append({\"page_content\": d.page_content, \"metadata\": d.metadata})\n",
        "\n",
        "    with open(DOC_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_docs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return len(all_docs)\n",
        "\n",
        "def load_docs():\n",
        "    if not os.path.exists(DOC_PATH):\n",
        "        return []\n",
        "    try:\n",
        "        with open(DOC_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# ------------------- Streamlit UI -------------------\n",
        "st.set_page_config(page_title=\"Web ‚Üí Local JSON + QA Agent\", layout=\"wide\")\n",
        "st.title(\"üï∏ Web ‚Üí Local JSON + Local QA Agent\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"üï∑ Crawl & Store\", \"ü§ñ Ask the Agent\"])\n",
        "\n",
        "# --------- Tab 1: Crawl ----------\n",
        "with tab1:\n",
        "    st.header(\"Crawl a page or site and store as local documents (JSON + Markdown)\")\n",
        "    crawl_mode = st.radio(\"Extraction Type:\", [\"Single Page\", \"Whole Website\"])\n",
        "    url = st.text_input(\"Enter URL:\", value=\"https://www.bbc.com/\")\n",
        "    save_mode = st.radio(\"When saving:\", [\"Overwrite JSON file\", \"Append to JSON file\"])\n",
        "    do_crawl = st.button(\"üï∑ Crawl & Save\")\n",
        "\n",
        "    if do_crawl:\n",
        "        if not url.startswith(\"http\"):\n",
        "            st.warning(\"Enter a valid URL starting with http/https.\")\n",
        "        else:\n",
        "            max_pages = 1 if crawl_mode == \"Single Page\" else 5\n",
        "            with st.spinner(\"Crawling website...\"):\n",
        "                try:\n",
        "                    data = asyncio.run(crawl_site_async(url, max_pages))\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error: {e}\")\n",
        "                    st.error(traceback.format_exc())\n",
        "                    data = None\n",
        "\n",
        "            if data:\n",
        "                pages = data.pages if hasattr(data, \"pages\") else [data]\n",
        "                docs = []\n",
        "                for p in pages:\n",
        "                    md_text = getattr(p, \"markdown\", None)\n",
        "                    if not md_text:\n",
        "                        md_text = markdownify.markdownify(getattr(p, \"content\", \"\") or \"\")\n",
        "\n",
        "                    # Clean\n",
        "                    clean = []\n",
        "                    for line in md_text.splitlines():\n",
        "                        line = line.strip()\n",
        "                        if not line: continue\n",
        "                        if any(x in line.lower() for x in [\"copyright\", \"cookie\", \"advertisement\", \"accept\"]):\n",
        "                            continue\n",
        "                        clean.append(line)\n",
        "                    md_text = \"\\n\".join(clean)\n",
        "\n",
        "                    # Save .md locally\n",
        "                    fname = os.path.join(MD_DIR, f\"page_{len(os.listdir(MD_DIR)) + 1}.md\")\n",
        "                    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(md_text)\n",
        "\n",
        "                    docs.append(Document(page_content=md_text, metadata={\"source\": p.url if hasattr(p, \"url\") else url}))\n",
        "\n",
        "                total = save_as_json(docs, append=(save_mode == \"Append to JSON file\"))\n",
        "                st.success(f\"‚úÖ Saved {len(docs)} page(s). Total stored documents: {total}\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"üì¶ Local Data\")\n",
        "\n",
        "    stored = load_docs()\n",
        "    st.write(f\"üßæ Documents stored: *{len(stored)}*\")\n",
        "\n",
        "    # ‚úÖ Download JSON\n",
        "    if os.path.exists(DOC_PATH):\n",
        "        with open(DOC_PATH, \"rb\") as f:\n",
        "            st.download_button(\"‚¨á Download stored_docs.json\", f, \"stored_docs.json\", \"application/json\")\n",
        "\n",
        "    # ‚úÖ Download ZIP of Markdown files\n",
        "    if len(os.listdir(MD_DIR)) > 0:\n",
        "        buffer = io.BytesIO()\n",
        "        with zipfile.ZipFile(buffer, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "            for filename in os.listdir(MD_DIR):\n",
        "                z.write(os.path.join(MD_DIR, filename), filename)\n",
        "        buffer.seek(0)\n",
        "        st.download_button(\"‚¨á Download Markdown ZIP\", buffer, \"scraped_markdown.zip\", \"application/zip\")\n",
        "\n",
        "    # Preview\n",
        "    if stored:\n",
        "        preview = st.slider(\"Preview how many docs:\", 1, min(10, len(stored)), 3)\n",
        "        for i, d in enumerate(stored[:preview]):\n",
        "            st.markdown(f\"### Doc {i+1} ‚Äî Source: {d['metadata'].get('source','-')}\")\n",
        "            st.code(d[\"page_content\"][:500] + \"...\", language=\"markdown\")\n",
        "\n",
        "# --------- Tab 2: QA Agent ----------\n",
        "with tab2:\n",
        "    st.header(\"Ask the local QA agent (NO internet, answers from stored documents only)\")\n",
        "    docs = load_docs()\n",
        "    query = st.text_input(\"Enter your question:\")\n",
        "    ask = st.button(\"üîç Ask\")\n",
        "\n",
        "    if ask:\n",
        "        if not docs:\n",
        "            st.warning(\"No documents available. Crawl first.\")\n",
        "        else:\n",
        "            with st.spinner(\"Generating answer...\"):\n",
        "                context = \"\\n\\n\".join(d[\"page_content\"] for d in docs)[:7000]\n",
        "                model_name = \"google/flan-t5-small\"\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "                qa = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "                prompt = f\"Use only the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nIf not in context reply: ‚ùå Not found.\"\n",
        "                out = qa(prompt)[0]['generated_text']\n",
        "                st.success(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWGdQw4bRWt3",
        "outputId": "340f8cda-12d5-4fb2-c1e0-8a8df7a22077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_local_agent_fixed.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - Launch Streamlit + ngrok (safe launcher)\n",
        "import os, time, psutil, signal, threading, socket, subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1) find the app file we just wrote\n",
        "APP_FILE = \"app_local_agent_fixed.py\"\n",
        "if not os.path.exists(APP_FILE):\n",
        "    raise FileNotFoundError(f\"App file not found: {APP_FILE}\")\n",
        "\n",
        "# 2) kill previous streamlit/ngrok processes\n",
        "for p in psutil.process_iter(['pid', 'name']):\n",
        "    try:\n",
        "        if p.info['name'] and ('streamlit' in p.info['name'] or 'ngrok' in p.info['name']):\n",
        "            os.kill(p.info['pid'], signal.SIGTERM)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 3) (optional) set ngrok token\n",
        "NGROK_TOKEN = \"2xRzSISIlaRrLMhqAs5DFLKOqB8_2Y1Q9CiCFYJas4Mes6fnp\"  # <-- put your token here if you have one (recommended)\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"STREAMLIT_SERVER_HEADLESS\"] = \"true\"\n",
        "cmd = [\"streamlit\", \"run\", APP_FILE, \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\"]\n",
        "proc = subprocess.Popen(cmd, env=env)\n",
        "\n",
        "def wait_for_port(port, timeout=120):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((\"127.0.0.1\", port), timeout=2):\n",
        "                return True\n",
        "        except OSError:\n",
        "            time.sleep(2)\n",
        "    return False\n",
        "\n",
        "print(\"‚è≥ Waiting for Streamlit to start (up to 120s)...\")\n",
        "if wait_for_port(8501, timeout=120):\n",
        "    print(\"‚úÖ Streamlit started on localhost:8501\")\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"üåç Public app URL:\", public_url.public_url)\n",
        "    print(\"üîó (If the page shows ERR_NGROK_8012, wait a few seconds and refresh.)\")\n",
        "else:\n",
        "    print(\"‚ùå Streamlit did not start in time. Check logs with: !streamlit logs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmMJEvnKReM5",
        "outputId": "c39750b4-f19e-4458-967d-19fa16a7e18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Waiting for Streamlit to start (up to 120s)...\n",
            "‚úÖ Streamlit started on localhost:8501\n",
            "üåç Public app URL: https://28ad1100f191.ngrok-free.app\n",
            "üîó (If the page shows ERR_NGROK_8012, wait a few seconds and refresh.)\n"
          ]
        }
      ]
    }
  ]
}